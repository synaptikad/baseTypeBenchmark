# BaseType Benchmark - Makefile

.PHONY: help install test demo dataset benchmark clean hf-check hf-login hf-publish hf-publish-dry hf-publish-custom

# Variables
PYTHON := python3
PIP := pip3
SRC_DIR := src
SCRIPT := $(SRC_DIR)/scripts/basetype_benchmark.py

# Couleurs pour les messages
GREEN := \033[0;32m
BLUE := \033[0;34m
YELLOW := \033[1;33m
RED := \033[0;31m
NC := \033[0m # No Color

# Aide
help: ## Afficher cette aide
	@echo "$(BLUE)BaseType Benchmark - Syst√®me autonome$(NC)"
	@echo ""
	@echo "Commandes disponibles:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  $(GREEN)%-20s$(NC) %s\n", $$1, $$2}'

# Setup initial (nouvelle machine)
# Note: make doit etre installe manuellement d'abord:
#   sudo apt update && sudo apt install -y make
init: ## Setup initial complet (Docker, Python, deps)
	@echo "$(BLUE)Setup initial...$(NC)"
	@echo "Installation Docker et dependances systeme..."
	sudo apt update && sudo apt install -y docker.io docker-compose-v2 python3-pip python3-venv git
	sudo usermod -aG docker $$USER
	@echo "$(YELLOW)Reconnectez-vous ou executez: newgrp docker$(NC)"
	@echo "$(GREEN)Setup systeme termine. Executez ensuite: make install$(NC)"

# Installation
install: ## Installer les d√©pendances Python
	@echo "$(BLUE)Installation des d√©pendances...$(NC)"
	$(PIP) install -e .
	$(PIP) install -r requirements.txt

install-dev: ## Installer les d√©pendances de d√©veloppement
	$(PIP) install -e ".[dev]"
	$(PIP) install -r requirements.txt

# Tests et validation
test: ## Ex√©cuter les tests unitaires
	@echo "$(BLUE)Ex√©cution des tests...$(NC)"
	$(PYTHON) -m pytest tests/ -v

test-integration: ## Tests d'int√©gration (n√©cessite Docker)
	@echo "$(YELLOW)Tests d'int√©gration (avec Docker)...$(NC)"
	$(PYTHON) -c "from basetype_benchmark.dataset.workflow import DatasetWorkflow; wf = DatasetWorkflow(); print('‚úÖ Imports OK')"

demo: ## D√©monstration de l'autonomie du syst√®me
	@echo "$(BLUE)D√©monstration autonomie syst√®me...$(NC)"
	$(PYTHON) demo_autonomie.py

# Gestion des datasets
dataset-storage: ## Afficher l'√©tat du stockage
	@echo "$(BLUE)√âtat du stockage Codespace:$(NC)"
	$(PYTHON) $(SCRIPT) dataset storage

dataset-generate: ## G√©n√©rer un dataset de test (small-1w)
	@echo "$(YELLOW)G√©n√©ration dataset small-1w...$(NC)"
	$(PYTHON) $(SCRIPT) workflow session small-1w

dataset-sequential: ## G√©n√©ration s√©quentielle automatique
	@echo "$(YELLOW)D√©marrage g√©n√©ration s√©quentielle...$(NC)"
	@echo "$(RED)‚ö†Ô∏è  Cette commande peut prendre du temps et utiliser de l'espace$(NC)"
	@read -p "Continuer ? (y/N) " confirm && [ "$$confirm" = "y" ] || exit 1
	$(PYTHON) $(SCRIPT) workflow sequential

# Benchmarks
benchmark-test: ## Test rapide (small-1w √ó postgres)
	@echo "$(YELLOW)Test benchmark rapide...$(NC)"
	$(PYTHON) $(SCRIPT) benchmark single small-1w postgres

benchmark-full: ## Suite compl√®te de benchmarks (CONFIRMATION REQUISE)
	@echo "$(RED)üö® SUITE COMPL√àTE DE BENCHMARKS$(NC)"
	@echo "$(RED)   ‚Ä¢ 108 tests combinaisons$(NC)"
	@echo "$(RED)   ‚Ä¢ Plusieurs heures d'ex√©cution$(NC)"
	@echo "$(RED)   ‚Ä¢ Ressources Docker n√©cessaires$(NC)"
	@echo ""
	@read -p "√ätes-vous s√ªr de vouloir continuer ? (y/N) " confirm && [ "$$confirm" = "y" ] || exit 1
	@echo "$(BLUE)D√©marrage suite compl√®te...$(NC)"
	$(PYTHON) $(SCRIPT) benchmark full-suite

benchmark-list: ## Lister les profils de benchmark disponibles
	@echo "$(BLUE)Profils de benchmark disponibles:$(NC)"
	$(PYTHON) $(SCRIPT) benchmark list

# Benchmarks acad√©miques s√©quentiels (nouveau workflow rigoureux)
benchmark-sequential-info: ## Afficher les profils compatibles pour workflow s√©quentiel
	@echo "$(BLUE)Profils compatibles workflow s√©quentiel acad√©mique:$(NC)"
	$(PYTHON) $(SRC_DIR)/scripts/run_sequential_benchmark.py info

benchmark-sequential-single: ## Benchmark s√©quentiel d'un profil (usage: make benchmark-sequential-single PROFILE=small-1w)
	@echo "$(YELLOW)Workflow s√©quentiel pour $(PROFILE)...$(NC)"
	$(PYTHON) $(SRC_DIR)/scripts/run_sequential_benchmark.py single $(PROFILE)

benchmark-sequential-suite: ## Suite s√©quentielle compl√®te (5 profils √ó 3 paradigmes = 15 benchmarks)
	@echo "$(RED)üö® SUITE S√âQUENTIELLE ACAD√âMIQUE$(NC)"
	@echo "$(YELLOW)   ‚Ä¢ 5 profils √ó 3 paradigmes = 15 benchmarks$(NC)"
	@echo "$(YELLOW)   ‚Ä¢ G√©n√©ration d√©terministe (seed=42)$(NC)"
	@echo "$(YELLOW)   ‚Ä¢ Tests s√©quentiels sur m√™me dataset$(NC)"
	@echo "$(YELLOW)   ‚Ä¢ Plusieurs heures d'ex√©cution$(NC)"
	@echo ""
	@read -p "Continuer ? (y/N) " confirm && [ "$$confirm" = "y" ] || exit 1
	@echo "$(BLUE)D√©marrage suite s√©quentielle...$(NC)"
	$(PYTHON) $(SRC_DIR)/scripts/run_sequential_benchmark.py suite

# HuggingFace Hub (publication acad√©mique)
hf-check: ## V√©rifier la configuration HuggingFace
	@echo "$(BLUE)V√©rification HuggingFace...$(NC)"
	@$(PYTHON) -c "from basetype_benchmark.dataset.huggingface import check_dependencies; deps = check_dependencies(); print('huggingface_hub:', '‚úÖ' if deps['huggingface_hub'] else '‚ùå'); print('pyarrow:', '‚úÖ' if deps['pyarrow'] else '‚ùå')"
	@if [ -z "$$HF_TOKEN" ]; then echo "$(YELLOW)‚ö†Ô∏è  HF_TOKEN non d√©fini$(NC)"; else echo "$(GREEN)‚úÖ HF_TOKEN configur√©$(NC)"; fi

hf-login: ## Se connecter √† HuggingFace (interactif)
	@echo "$(BLUE)Connexion √† HuggingFace Hub...$(NC)"
	huggingface-cli login

hf-publish-dry: ## G√©n√©rer le dataset sans publier (test local)
	@echo "$(YELLOW)G√©n√©ration dataset pour HuggingFace (dry-run)...$(NC)"
	$(PYTHON) $(SRC_DIR)/scripts/publish_to_huggingface.py --profile=large-1y --skip-publish

hf-publish: ## Publier le dataset sur HuggingFace Hub
	@echo "$(RED)üöÄ PUBLICATION SUR HUGGINGFACE HUB$(NC)"
	@echo "$(YELLOW)   ‚Ä¢ Profil: large-1y (dataset complet)$(NC)"
	@echo "$(YELLOW)   ‚Ä¢ Repo: synaptikad/basetype-benchmark$(NC)"
	@echo "$(YELLOW)   ‚Ä¢ G√©n√©ration + upload$(NC)"
	@echo ""
	@if [ -z "$$HF_TOKEN" ]; then echo "$(RED)‚ùå HF_TOKEN requis. Configurez avec: export HF_TOKEN=hf_xxx$(NC)"; exit 1; fi
	@read -p "Publier sur HuggingFace ? (y/N) " confirm && [ "$$confirm" = "y" ] || exit 1
	$(PYTHON) $(SRC_DIR)/scripts/publish_to_huggingface.py --profile=large-1y

hf-publish-custom: ## Publier avec profil personnalis√© (usage: make hf-publish-custom PROFILE=medium-1m)
	@echo "$(YELLOW)Publication profil $(PROFILE)...$(NC)"
	@if [ -z "$$HF_TOKEN" ]; then echo "$(RED)‚ùå HF_TOKEN requis$(NC)"; exit 1; fi
	$(PYTHON) $(SRC_DIR)/scripts/publish_to_huggingface.py --profile=$(PROFILE)

# Nettoyage
clean: ## Nettoyer les caches et fichiers temporaires
	@echo "$(YELLOW)Nettoyage des caches...$(NC)"
	find . -name "*.pyc" -delete
	find . -name "__pycache__" -type d -exec rm -rf {} +
	find . -name "*.cache" -delete
	@echo "$(GREEN)Nettoyage termin√©$(NC)"

clean-data: ## Nettoyer les datasets g√©n√©r√©s (ATTENTION)
	@echo "$(RED)‚ö†Ô∏è  Cette commande supprime TOUS les datasets g√©n√©r√©s$(NC)"
	@read -p "Continuer ? (y/N) " confirm && [ "$$confirm" = "y" ] || exit 1
	rm -rf data/
	@echo "$(GREEN)Datasets supprim√©s$(NC)"

# Docker
docker-build: ## Construire les images Docker
	@echo "$(BLUE)Construction images Docker...$(NC)"
	cd docker && docker-compose build

docker-up: ## D√©marrer les services Docker
	@echo "$(BLUE)D√©marrage services Docker...$(NC)"
	cd docker && docker-compose up -d

docker-down: ## Arr√™ter les services Docker
	@echo "$(BLUE)Arr√™t services Docker...$(NC)"
	cd docker && docker-compose down

docker-logs: ## Afficher les logs Docker
	cd docker && docker-compose logs -f

# D√©veloppement
lint: ## V√©rifier le style du code
	@echo "$(BLUE)V√©rification du code...$(NC)"
	$(PYTHON) -m black --check src/
	$(PYTHON) -m isort --check-only src/

format: ## Formatter le code
	@echo "$(BLUE)Formatage du code...$(NC)"
	$(PYTHON) -m black src/
	$(PYTHON) -m isort src/

# Informations syst√®me
info: ## Informations sur le syst√®me
	@echo "$(BLUE)Informations syst√®me:$(NC)"
	@echo "Python: $$(python3 --version)"
	@echo "Docker: $$(docker --version 2>/dev/null || echo 'Non install√©')"
	@echo "Espace disque: $$(df -h . | tail -1 | awk '{print $$4 \" libres\"}')"
	@echo ""
	@echo "$(BLUE)√âtat du projet:$(NC)"
	@ls -la | grep -E "\.(md|txt|toml)$$" | while read line; do echo "  $$line"; done

# Raccourcis pratiques
setup: install docker-build ## Installation compl√®te
	@echo "$(GREEN)Installation termin√©e !$(NC)"
	@echo "Utilisez 'make demo' pour voir l'autonomie du syst√®me"

quick-start: dataset-storage dataset-generate benchmark-test ## D√©marrage rapide (test complet)
	@echo "$(GREEN)D√©marrage rapide termin√© !$(NC)"

# S√©curit√©
check-security: ## V√©rifications de s√©curit√© basiques
	@echo "$(BLUE)V√©rifications de s√©curit√©...$(NC)"
	@echo "üîç Recherche de mots de passe en dur..."
	@grep -r "password\|token\|secret" --include="*.py" src/ || echo "‚úÖ Aucun mot de passe trouv√©"
	@echo "üîç Recherche de cl√©s API..."
	@grep -r "api_key\|API_KEY" --include="*.py" src/ || echo "‚úÖ Aucune cl√© API en dur"

# Documentation
docs: ## G√©n√©rer la documentation
	@echo "$(BLUE)G√©n√©ration documentation...$(NC)"
	@echo "üìñ README principal: README.md"
	@echo "üìñ Docs dataset: src/basetype_benchmark/dataset/README.md"
	@echo "üîó Structure du code dans les docstrings"

# Alias pour compatibilit√©
all: help
.DEFAULT_GOAL := help
