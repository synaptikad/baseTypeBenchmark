#!/usr/bin/env python3
"""Script principal de lancement du benchmark BaseType."""
from __future__ import annotations

import argparse
import sys
from pathlib import Path

# Ajouter src au path pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from basetype_benchmark.dataset.workflow import DatasetWorkflow
from basetype_benchmark.dataset.orchestrator import BenchmarkOrchestrator


def main():
    """Point d'entr√©e principal."""
    parser = argparse.ArgumentParser(
        description="BaseType Benchmark - Comparaison des paradigmes de BD pour donn√©es b√¢timentaires"
    )

    subparsers = parser.add_subparsers(dest='command', help='Commandes disponibles')

    # Commande dataset
    dataset_parser = subparsers.add_parser('dataset', help='Gestion des datasets')
    dataset_parser.add_argument('action', choices=['generate', 'list', 'clean', 'storage'],
                               help='Action √† effectuer')
    dataset_parser.add_argument('--profile', help='Profil de dataset')
    dataset_parser.add_argument('--seed', type=int, default=42, help='Seed pour g√©n√©ration')

    # Commande benchmark
    benchmark_parser = subparsers.add_parser('benchmark', help='Ex√©cution des benchmarks')
    benchmark_parser.add_argument('action', choices=['run', 'full-suite', 'single'],
                                 help='Type de benchmark')
    benchmark_parser.add_argument('--profile', help='Profil pour test unique')
    benchmark_parser.add_argument('--model', choices=['postgres', 'memgraph', 'oxigraph'],
                                 help='Mod√®le pour test unique')

    # Commande workflow
    workflow_parser = subparsers.add_parser('workflow', help='Workflows automatis√©s')
    workflow_parser.add_argument('action', choices=['sequential', 'smart-select', 'session'],
                                help='Type de workflow')
    workflow_parser.add_argument('--profiles', nargs='*', help='Profils pour g√©n√©ration s√©quentielle')
    workflow_parser.add_argument('--max-gb', type=float, default=10, help='Espace max pour s√©lection')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    try:
        if args.command == 'dataset':
            handle_dataset_command(args)
        elif args.command == 'benchmark':
            handle_benchmark_command(args)
        elif args.command == 'workflow':
            handle_workflow_command(args)
        else:
            parser.print_help()

    except Exception as e:
        print(f"[ERROR] Erreur: {e}")
        sys.exit(1)


def handle_dataset_command(args):
    """Gestion des commandes dataset."""
    workflow = DatasetWorkflow()

    if args.action == 'generate':
        if not args.profile:
            print("[ERROR] Profil requis pour g√©n√©ration")
            return
        success = workflow.benchmark_session_workflow(args.profile, args.seed)
        print(f"[OK] G√©n√©ration {'r√©ussie' if success else '√©chou√©e'}")

    elif args.action == 'list':
        # TODO: Impl√©menter listing des datasets
        print("üìã Listing des datasets (TODO)")

    elif args.action == 'clean':
        # TODO: Impl√©menter nettoyage
        print("üßπ Nettoyage (TODO)")

    elif args.action == 'storage':
        storage = workflow.get_codespace_storage_info()
        if 'error' in storage:
            print(f"[ERROR] {storage['error']}")
        else:
            print("üíæ STOCKAGE CODESPACE")
            print(f"Total: {storage['total_gb']} GB")
            print(f"Utilis√©: {storage['used_gb']} GB")
            print(f"Libre: {storage['free_gb']} GB")
            print(f"Usage: {storage['usage_percent']}%")


def handle_benchmark_command(args):
    """Gestion des commandes benchmark."""
    orchestrator = BenchmarkOrchestrator()

    if args.action == 'run':
        print("üèÉ Ex√©cution benchmark (TODO)")
    elif args.action == 'full-suite':
        results = orchestrator.run_full_benchmark_suite()
        print(f"[DONE] Suite termin√©e: {results['successful_tests']}/{results['total_tests']} r√©ussis")
    elif args.action == 'single':
        if not args.profile or not args.model:
            print("[ERROR] Profil et mod√®le requis pour test unique")
            return
        result = orchestrator.run_single_benchmark(args.profile, args.model)
        status = "[OK] R√âUSSI" if result.success else "[ERROR] √âCHEC"
        print(f"{status}: {result.profile} √ó {result.model} ({result.duration_seconds:.1f}s)")


def handle_workflow_command(args):
    """Gestion des commandes workflow."""
    workflow = DatasetWorkflow()

    if args.action == 'sequential':
        if not args.profiles:
            # S√©lection automatique
            storage = workflow.get_codespace_storage_info()
            if 'free_gb' in storage:
                max_gb = min(storage['free_gb'] * 0.8, 10.0)
                args.profiles = workflow.smart_profile_selection(max_gb)
            else:
                args.profiles = ['small-1w', 'small-1m']

        print(f"[START] G√©n√©ration s√©quentielle: {', '.join(args.profiles)}")
        results = workflow.sequential_generation_workflow(args.profiles)
        print(f"[OK] Termin√©: {results['successful']}/{results['total']} r√©ussis")

    elif args.action == 'smart-select':
        selected = workflow.smart_profile_selection(args.max_gb)
        print(f"üéØ Profils s√©lectionn√©s ({args.max_gb} GB max): {', '.join(selected)}")

    elif args.action == 'session':
        if not args.profiles or len(args.profiles) != 1:
            print("[ERROR] Un seul profil requis pour session")
            return
        success = workflow.benchmark_session_workflow(args.profiles[0])
        print(f"[OK] Session {'r√©ussie' if success else '√©chou√©e'}")


if __name__ == "__main__":
    main()