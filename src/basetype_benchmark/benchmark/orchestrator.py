#!/usr/bin/env python3
"""Orchestrateur de benchmark rigoureux pour garantir la validit√© acad√©mique.

Ce module impl√©mente un workflow s√©quentiel strict qui garantit:
- Un seul dataset g√©n√©r√© (seed=42) par profil
- Export dans les 3 formats (SQL, graph, RDF)
- Tests s√©quentiels sur les 3 paradigmes avec le M√äME dataset
- Isolation compl√®te entre tests (nettoyage des conteneurs)
- Collecte standardis√©e des m√©triques (JSON)

Workflow acad√©mique:
    FOR EACH profile IN [small-1w, small-1m, ..., large-1y]:
        1. G√©n√©rer dataset (seed=42)
        2. Exporter vers PostgreSQL, Memgraph, Oxigraph
        3. FOR EACH paradigm IN [postgres, memgraph, oxigraph]:
            a. D√©marrer conteneur propre
            b. Charger dataset
            c. Ex√©cuter Q1-Q8
            d. Collecter m√©triques (latency, RAM, CPU, disk)
            e. Arr√™ter et nettoyer conteneur
        4. Sauvegarder r√©sultats JSON
        5. Nettoyer cache dataset (optionnel)

Compatible avec:
    - Codespace 32GB RAM: 5 profils
    - Codespace 64GB RAM: 9 profils
    - OVH B3-256 (256GB RAM): 12 profils (TOUS)
"""
from __future__ import annotations

import json
import os
import subprocess
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from basetype_benchmark.dataset.config import PROFILES, DEFAULT_SEED


# =============================================================================
# Dataclasses pour les r√©sultats
# =============================================================================

@dataclass
class QueryMetrics:
    """M√©triques pour une requ√™te."""
    query_id: str
    latency_p50_ms: float
    latency_p95_ms: float
    latency_max_ms: float
    latency_min_ms: float
    rows_returned: int
    runs: int
    error: Optional[str] = None


@dataclass
class ParadigmBenchmark:
    """R√©sultats benchmark pour un paradigm sur un dataset."""
    paradigm: str  # postgres, memgraph, oxigraph
    profile: str
    seed: int

    # M√©triques d'ingestion
    load_duration_s: float
    load_ram_peak_mb: float
    load_disk_mb: float
    load_items: int

    # M√©triques de queries (Q1-Q8)
    queries: List[Dict[str, Any]]

    # M√©triques syst√®me pendant queries
    query_ram_steady_mb: float
    query_ram_peak_mb: float
    query_cpu_avg_percent: float

    # M√©tadonn√©es
    timestamp: str
    docker_image: str
    container_id: str
    engine: str


@dataclass
class DatasetBenchmarkSession:
    """Session compl√®te pour un dataset sur les 3 paradigmes."""
    profile: str
    seed: int
    timestamp: str

    # M√©tadonn√©es dataset
    dataset_nodes: int
    dataset_edges: int
    dataset_timeseries_points: int

    # R√©sultats des 3 paradigmes
    postgres: Optional[Dict] = None
    memgraph: Optional[Dict] = None
    oxigraph: Optional[Dict] = None

    # Statut global
    status: str = "pending"  # pending, in_progress, completed, failed
    errors: List[str] = field(default_factory=list)


# =============================================================================
# Constantes de configuration
# =============================================================================

DOCKER_COMPOSE_FILE = "docker/docker-compose.yml"

# Mapping paradigm -> service docker-compose
PARADIGM_SERVICES = {
    "postgres": "timescaledb",
    "memgraph": "memgraph",
    "oxigraph": "oxigraph",
}

# Mapping paradigm -> container name
PARADIGM_CONTAINERS = {
    "postgres": "btb_timescaledb",
    "memgraph": "btb_memgraph",
    "oxigraph": "btb_oxigraph",
}

# Mapping paradigm -> runner profile YAML
PARADIGM_PROFILES = {
    "postgres": "pg_rel",
    "memgraph": "memgraph",
    "oxigraph": "oxigraph",
}

# Health check timeouts
HEALTH_CHECK_TIMEOUT = 120  # seconds
HEALTH_CHECK_INTERVAL = 5  # seconds


# =============================================================================
# Classe principale
# =============================================================================

class BenchmarkOrchestrator:
    """Orchestrateur de benchmark rigoureux."""

    def __init__(self, output_dir: Path = None, docker_compose_file: str = None):
        """Initialise l'orchestrateur.

        Args:
            output_dir: R√©pertoire de sortie des r√©sultats (d√©faut: ./benchmark_results)
            docker_compose_file: Fichier docker-compose (d√©faut: docker/docker-compose.yml)
        """
        self.output_dir = output_dir or Path("./benchmark_results")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.data_dir = Path("./data")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        self.docker_compose_file = docker_compose_file or DOCKER_COMPOSE_FILE
        self.cwd = Path.cwd()

        # V√©rifier Docker disponible
        self._check_docker()

    def _check_docker(self) -> None:
        """V√©rifie que Docker est disponible."""
        try:
            result = subprocess.run(
                ["docker", "--version"],
                capture_output=True,
                text=True,
                check=True
            )
            print(f"[OK] Docker disponible: {result.stdout.strip()}")
        except (subprocess.CalledProcessError, FileNotFoundError):
            raise RuntimeError("[ERROR] Docker n'est pas disponible. Requis pour les benchmarks.")

    def _get_storage_info(self) -> Dict[str, float]:
        """R√©cup√®re les informations de stockage."""
        try:
            stat = os.statvfs('/')
            total_gb = (stat.f_blocks * stat.f_frsize) / (1024**3)
            free_gb = (stat.f_available * stat.f_frsize) / (1024**3)
            used_gb = total_gb - free_gb

            return {
                'total_gb': round(total_gb, 1),
                'used_gb': round(used_gb, 1),
                'free_gb': round(free_gb, 1),
                'usage_percent': round((used_gb / total_gb) * 100, 1)
            }
        except Exception:
            return {'error': 'Unknown'}

    def _get_ram_info(self) -> Dict[str, float]:
        """R√©cup√®re les informations RAM."""
        try:
            with open('/proc/meminfo', 'r') as f:
                meminfo = f.read()
            total_kb = int([l for l in meminfo.split('\n') if 'MemTotal' in l][0].split()[1])
            return {'total_gb': round(total_kb / 1024 / 1024, 1)}
        except Exception:
            return {'total_gb': 32}  # Default fallback

    # =========================================================================
    # Docker management
    # =========================================================================

    def _docker_compose(self, *args, check: bool = True) -> subprocess.CompletedProcess:
        """Ex√©cute une commande docker-compose."""
        cmd = ["docker", "compose", "-f", self.docker_compose_file] + list(args)
        return subprocess.run(
            cmd,
            cwd=self.cwd,
            capture_output=True,
            text=True,
            check=check
        )

    def _start_service(self, service: str) -> bool:
        """D√©marre un service Docker."""
        print(f"  üê≥ D√©marrage {service}...")
        try:
            self._docker_compose("up", "-d", service)
            return True
        except subprocess.CalledProcessError as e:
            print(f"  [ERROR] Erreur d√©marrage {service}: {e.stderr}")
            return False

    def _stop_service(self, service: str, remove_volumes: bool = True) -> bool:
        """Arr√™te et nettoie un service Docker."""
        print(f"  üõë Arr√™t {service}...")
        try:
            if remove_volumes:
                self._docker_compose("down", "-v", "--remove-orphans")
            else:
                self._docker_compose("down")
            return True
        except subprocess.CalledProcessError as e:
            print(f"  [WARN]  Erreur arr√™t {service}: {e.stderr}")
            return False

    def _wait_for_health(self, container: str, timeout: int = HEALTH_CHECK_TIMEOUT) -> bool:
        """Attend qu'un conteneur soit healthy."""
        print(f"  ‚è≥ Attente health check {container}...")
        start = time.time()

        while time.time() - start < timeout:
            try:
                result = subprocess.run(
                    ["docker", "inspect", "-f", "{{.State.Health.Status}}", container],
                    capture_output=True,
                    text=True,
                    check=True
                )
                status = result.stdout.strip()
                if status == "healthy":
                    print(f"  [OK] {container} healthy")
                    return True
                elif status == "unhealthy":
                    print(f"  [ERROR] {container} unhealthy")
                    return False
            except subprocess.CalledProcessError:
                pass  # Container not ready yet

            time.sleep(HEALTH_CHECK_INTERVAL)

        print(f"  [ERROR] Timeout waiting for {container}")
        return False

    def _get_container_id(self, container: str) -> str:
        """R√©cup√®re l'ID d'un conteneur."""
        try:
            result = subprocess.run(
                ["docker", "inspect", "-f", "{{.Id}}", container],
                capture_output=True,
                text=True,
                check=True
            )
            return result.stdout.strip()[:12]
        except subprocess.CalledProcessError:
            return "unknown"

    # =========================================================================
    # Dataset generation
    # =========================================================================

    def generate_and_export_dataset(self, profile: str, seed: int = DEFAULT_SEED) -> bool:
        """G√©n√®re un dataset et l'exporte dans tous les formats.

        Args:
            profile: Nom du profil (small-1w, medium-1m, etc.)
            seed: Graine al√©atoire (d√©faut: 42)

        Returns:
            True si succ√®s, False sinon
        """
        print(f"\n{'='*60}")
        print(f"[INFO] G√âN√âRATION DATASET: {profile} (seed={seed})")
        print(f"{'='*60}")

        # V√©rifier espace disponible
        storage = self._get_storage_info()
        if 'free_gb' in storage and storage['free_gb'] < 5:
            print(f"[WARN]  ATTENTION: Seulement {storage['free_gb']} GB libres")

        try:
            # G√©n√©rer dataset via le workflow existant
            from basetype_benchmark.dataset.workflow import DatasetWorkflow
            workflow = DatasetWorkflow()

            # Le workflow existant g√©n√®re et exporte automatiquement
            success = workflow.releaser.release_and_clean_workflow(
                profile,
                keep_local=True  # Garder pour les benchmarks
            )

            if success:
                print(f"[OK] Dataset {profile} g√©n√©r√© et export√©")
                return True
            else:
                print(f"[ERROR] √âchec g√©n√©ration {profile}")
                return False

        except Exception as e:
            print(f"[ERROR] ERREUR g√©n√©ration {profile}: {e}")
            return False

    # =========================================================================
    # Benchmark runners
    # =========================================================================

    def _run_benchmark_with_runner(self, runner_profile: str, container: str) -> Dict[str, Any]:
        """Ex√©cute le benchmark avec le runner existant.

        Args:
            runner_profile: Nom du profil YAML (pg_rel, memgraph, oxigraph)
            container: Nom du conteneur Docker

        Returns:
            Dict avec les r√©sultats du benchmark
        """
        from basetype_benchmark.benchmark.metrics import ResourceMonitor, latency_stats, volume_disk_usage

        # Charger le profil
        profiles_dir = Path("src/basetype_benchmark/benchmark/profiles")
        profile_path = profiles_dir / f"{runner_profile}.yaml"

        if not profile_path.exists():
            raise FileNotFoundError(f"Profil {runner_profile} non trouv√©: {profile_path}")

        import yaml
        with open(profile_path) as f:
            profile_data = yaml.safe_load(f)

        # D√©marrer monitoring
        monitor = ResourceMonitor(container)
        monitor.start()

        results = {
            "engine": profile_data.get("engine", runner_profile),
            "profile": runner_profile,
            "ingestion": {},
            "queries": [],
            "resources": {},
        }

        try:
            # Phase 1: Ingestion
            print(f"  üì• Phase ingestion...")
            ingestion_config = profile_data.get("ingestion", {})
            if ingestion_config and ingestion_config.get("command"):
                ingestion_start = time.perf_counter()
                cmd = ingestion_config["command"]
                subprocess.run(cmd, check=True, capture_output=True, text=True, cwd=self.cwd)
                ingestion_time = time.perf_counter() - ingestion_start
                results["ingestion"] = {"time_s": round(ingestion_time, 3)}
                print(f"  [OK] Ingestion: {ingestion_time:.1f}s")

            # Phase 2: Queries
            print(f"  üîç Phase queries...")
            queries_config = profile_data.get("queries", {})
            n_warmup = 3
            n_runs = 10

            for query_name, query_cmd in queries_config.items():
                # Handle different query formats (list, dict with selection/aggregation)
                if isinstance(query_cmd, dict):
                    # Complex query with selection + aggregation
                    print(f"    {query_name}: composite query (skipping for now)")
                    results["queries"].append({
                        "query": query_name,
                        "note": "composite query",
                        "stats": None
                    })
                    continue

                # Simple query
                print(f"    {query_name}...", end=" ", flush=True)

                # Warmup
                for _ in range(n_warmup):
                    try:
                        subprocess.run(query_cmd, check=True, capture_output=True, timeout=30)
                    except Exception:
                        pass

                # Measure
                latencies = []
                for _ in range(n_runs):
                    try:
                        t0 = time.perf_counter()
                        subprocess.run(query_cmd, check=True, capture_output=True, timeout=30)
                        latencies.append(time.perf_counter() - t0)
                    except Exception as e:
                        latencies.append(30.0)  # Timeout value

                stats = latency_stats(latencies)
                results["queries"].append({
                    "query": query_name,
                    "warmup_runs": n_warmup,
                    "measure_runs": latencies,
                    "stats": stats,
                })
                print(f"p50={stats['p50']:.3f}s p95={stats['p95']:.3f}s")

        finally:
            # Arr√™ter monitoring
            monitor.stop()
            resources = monitor.summarize()
            resources["volume_mb"] = volume_disk_usage(profile_data.get("volume"))
            results["resources"] = resources

        return results

    def _benchmark_postgres(self, profile: str, seed: int) -> Optional[ParadigmBenchmark]:
        """Benchmark PostgreSQL/TimescaleDB."""
        paradigm = "postgres"
        service = PARADIGM_SERVICES[paradigm]
        container = PARADIGM_CONTAINERS[paradigm]
        runner_profile = PARADIGM_PROFILES[paradigm]

        print(f"\nüìç Benchmark PostgreSQL/TimescaleDB")

        try:
            # D√©marrer conteneur
            if not self._start_service(service):
                return None

            # Attendre health
            if not self._wait_for_health(container):
                self._stop_service(service)
                return None

            # Ex√©cuter benchmark
            print(f"  [INFO] Ex√©cution benchmark...")
            results = self._run_benchmark_with_runner(runner_profile, container)

            # Construire ParadigmBenchmark
            benchmark = ParadigmBenchmark(
                paradigm=paradigm,
                profile=profile,
                seed=seed,
                load_duration_s=results.get("ingestion", {}).get("time_s", 0),
                load_ram_peak_mb=results.get("resources", {}).get("peak_mem_mb") or 0,
                load_disk_mb=results.get("resources", {}).get("volume_mb") or 0,
                load_items=results.get("ingestion", {}).get("items") or 0,
                queries=results.get("queries", []),
                query_ram_steady_mb=results.get("resources", {}).get("steady_state_mem_mb") or 0,
                query_ram_peak_mb=results.get("resources", {}).get("peak_mem_mb") or 0,
                query_cpu_avg_percent=results.get("resources", {}).get("avg_cpu_pct") or 0,
                timestamp=datetime.utcnow().isoformat() + "Z",
                docker_image="timescale/timescaledb-ha:pg16",
                container_id=self._get_container_id(container),
                engine=results.get("engine", "pg_rel"),
            )

            print(f"  [OK] PostgreSQL benchmark termin√©")
            return benchmark

        except Exception as e:
            print(f"  [ERROR] Erreur PostgreSQL: {e}")
            return None

        finally:
            # Toujours nettoyer
            self._stop_service(service, remove_volumes=True)

    def _benchmark_memgraph(self, profile: str, seed: int) -> Optional[ParadigmBenchmark]:
        """Benchmark Memgraph."""
        paradigm = "memgraph"
        service = PARADIGM_SERVICES[paradigm]
        container = PARADIGM_CONTAINERS[paradigm]
        runner_profile = PARADIGM_PROFILES[paradigm]

        print(f"\nüìç Benchmark Memgraph")

        try:
            # D√©marrer conteneur
            if not self._start_service(service):
                return None

            # Attendre health
            if not self._wait_for_health(container):
                self._stop_service(service)
                return None

            # Ex√©cuter benchmark
            print(f"  [INFO] Ex√©cution benchmark...")
            results = self._run_benchmark_with_runner(runner_profile, container)

            # Construire ParadigmBenchmark
            benchmark = ParadigmBenchmark(
                paradigm=paradigm,
                profile=profile,
                seed=seed,
                load_duration_s=results.get("ingestion", {}).get("time_s", 0),
                load_ram_peak_mb=results.get("resources", {}).get("peak_mem_mb") or 0,
                load_disk_mb=results.get("resources", {}).get("volume_mb") or 0,
                load_items=results.get("ingestion", {}).get("items") or 0,
                queries=results.get("queries", []),
                query_ram_steady_mb=results.get("resources", {}).get("steady_state_mem_mb") or 0,
                query_ram_peak_mb=results.get("resources", {}).get("peak_mem_mb") or 0,
                query_cpu_avg_percent=results.get("resources", {}).get("avg_cpu_pct") or 0,
                timestamp=datetime.utcnow().isoformat() + "Z",
                docker_image="memgraph/memgraph:latest",
                container_id=self._get_container_id(container),
                engine=results.get("engine", "memgraph"),
            )

            print(f"  [OK] Memgraph benchmark termin√©")
            return benchmark

        except Exception as e:
            print(f"  [ERROR] Erreur Memgraph: {e}")
            return None

        finally:
            # Toujours nettoyer
            self._stop_service(service, remove_volumes=True)

    def _benchmark_oxigraph(self, profile: str, seed: int) -> Optional[ParadigmBenchmark]:
        """Benchmark Oxigraph."""
        paradigm = "oxigraph"
        service = PARADIGM_SERVICES[paradigm]
        container = PARADIGM_CONTAINERS[paradigm]
        runner_profile = PARADIGM_PROFILES[paradigm]

        print(f"\nüìç Benchmark Oxigraph")

        try:
            # D√©marrer conteneur
            if not self._start_service(service):
                return None

            # Attendre health
            if not self._wait_for_health(container):
                self._stop_service(service)
                return None

            # Ex√©cuter benchmark
            print(f"  [INFO] Ex√©cution benchmark...")
            results = self._run_benchmark_with_runner(runner_profile, container)

            # Construire ParadigmBenchmark
            benchmark = ParadigmBenchmark(
                paradigm=paradigm,
                profile=profile,
                seed=seed,
                load_duration_s=results.get("ingestion", {}).get("time_s", 0),
                load_ram_peak_mb=results.get("resources", {}).get("peak_mem_mb") or 0,
                load_disk_mb=results.get("resources", {}).get("volume_mb") or 0,
                load_items=results.get("ingestion", {}).get("items") or 0,
                queries=results.get("queries", []),
                query_ram_steady_mb=results.get("resources", {}).get("steady_state_mem_mb") or 0,
                query_ram_peak_mb=results.get("resources", {}).get("peak_mem_mb") or 0,
                query_cpu_avg_percent=results.get("resources", {}).get("avg_cpu_pct") or 0,
                timestamp=datetime.utcnow().isoformat() + "Z",
                docker_image="oxigraph/oxigraph:latest",
                container_id=self._get_container_id(container),
                engine=results.get("engine", "oxigraph"),
            )

            print(f"  [OK] Oxigraph benchmark termin√©")
            return benchmark

        except Exception as e:
            print(f"  [ERROR] Erreur Oxigraph: {e}")
            return None

        finally:
            # Toujours nettoyer
            self._stop_service(service, remove_volumes=True)

    # =========================================================================
    # Main workflow
    # =========================================================================

    def run_paradigm_benchmark(
        self,
        profile: str,
        paradigm: str,
        seed: int = DEFAULT_SEED
    ) -> Optional[ParadigmBenchmark]:
        """Ex√©cute un benchmark complet pour un paradigme.

        Args:
            profile: Nom du profil
            paradigm: postgres, memgraph, ou oxigraph
            seed: Graine du dataset

        Returns:
            ParadigmBenchmark avec les m√©triques ou None si √©chec
        """
        print(f"\n{'‚îÄ'*60}")
        print(f"üî¨ BENCHMARK {paradigm.upper()}: {profile}")
        print(f"{'‚îÄ'*60}")

        if paradigm == "postgres":
            return self._benchmark_postgres(profile, seed)
        elif paradigm == "memgraph":
            return self._benchmark_memgraph(profile, seed)
        elif paradigm == "oxigraph":
            return self._benchmark_oxigraph(profile, seed)
        else:
            print(f"[ERROR] Paradigme inconnu: {paradigm}")
            return None

    def run_full_dataset_benchmark(
        self,
        profile: str,
        seed: int = DEFAULT_SEED,
        paradigms: List[str] = None
    ) -> DatasetBenchmarkSession:
        """Ex√©cute le workflow complet pour un dataset.

        Workflow:
        1. G√©n√©rer dataset (seed=42)
        2. Exporter vers tous les formats
        3. Tester s√©quentiellement les 3 paradigmes
        4. Collecter et sauvegarder r√©sultats

        Args:
            profile: Nom du profil (small-1w, etc.)
            seed: Graine al√©atoire
            paradigms: Liste des paradigmes √† tester (d√©faut: tous)

        Returns:
            DatasetBenchmarkSession avec tous les r√©sultats
        """
        if paradigms is None:
            paradigms = ["postgres", "memgraph", "oxigraph"]

        print(f"\n{'='*60}")
        print(f"[START] WORKFLOW BENCHMARK COMPLET: {profile}")
        print(f"{'='*60}")
        print(f"Paradigmes: {', '.join(paradigms)}")
        print(f"Seed: {seed}")

        # Cr√©er session
        session = DatasetBenchmarkSession(
            profile=profile,
            seed=seed,
            timestamp=time.strftime("%Y%m%d-%H%M%S"),
            dataset_nodes=0,
            dataset_edges=0,
            dataset_timeseries_points=0,
            status="in_progress"
        )

        try:
            # √âtape 1: G√©n√©ration et export
            print(f"\n[PACKAGE] √âtape 1/{len(paradigms)+1}: G√©n√©ration dataset")
            if not self.generate_and_export_dataset(profile, seed):
                session.status = "failed"
                session.errors.append("√âchec g√©n√©ration dataset")
                return session

            # √âtape 2+: Tests s√©quentiels des paradigmes
            for i, paradigm in enumerate(paradigms, start=2):
                print(f"\nüî¨ √âtape {i}/{len(paradigms)+1}: Test {paradigm}")

                result = self.run_paradigm_benchmark(profile, paradigm, seed)

                if result is not None:
                    # Convertir en dict pour JSON
                    setattr(session, paradigm, asdict(result))
                    print(f"[OK] {paradigm} termin√©")
                else:
                    error_msg = f"√âchec benchmark {paradigm}"
                    session.errors.append(error_msg)
                    print(f"[ERROR] {error_msg}")

                # Pause entre paradigmes pour lib√©rer ressources
                if i < len(paradigms) + 1:
                    print("‚è≥ Pause de 10s avant paradigme suivant...")
                    time.sleep(10)

            # Sauvegarder r√©sultats
            session.status = "completed" if not session.errors else "completed_with_errors"
            self._save_session(session)

        except Exception as e:
            session.status = "failed"
            session.errors.append(str(e))
            print(f"[ERROR] ERREUR CRITIQUE: {e}")

        return session

    def _save_session(self, session: DatasetBenchmarkSession) -> None:
        """Sauvegarde les r√©sultats d'une session."""
        output_file = self.output_dir / f"{session.profile}_{session.timestamp}.json"

        # Convertir en dict pour JSON
        session_dict = asdict(session)

        with output_file.open("w", encoding="utf-8") as f:
            json.dump(session_dict, f, indent=2, ensure_ascii=False)

        print(f"\nüíæ R√©sultats sauvegard√©s: {output_file}")

    def run_sequential_suite(
        self,
        profiles: List[str] = None,
        seed: int = DEFAULT_SEED,
        ram_gb: int = None
    ) -> List[DatasetBenchmarkSession]:
        """Ex√©cute la suite compl√®te de benchmarks s√©quentiels.

        Args:
            profiles: Liste des profils √† tester (d√©faut: selon RAM disponible)
            seed: Graine al√©atoire
            ram_gb: RAM disponible (auto-d√©tect√© si None)

        Returns:
            Liste des sessions de benchmark
        """
        # Auto-d√©tecter RAM si non sp√©cifi√©
        if ram_gb is None:
            ram_info = self._get_ram_info()
            ram_gb = int(ram_info.get('total_gb', 32))

        if profiles is None:
            # Profils par d√©faut selon RAM disponible
            if ram_gb >= 200:
                # OVH B3-256 ou √©quivalent: TOUS les profils
                profiles = [
                    "small-1w", "small-1m", "small-6m", "small-1y",
                    "medium-1w", "medium-1m", "medium-6m", "medium-1y",
                    "large-1w", "large-1m", "large-6m", "large-1y"
                ]
            elif ram_gb >= 64:
                # Codespace 64GB RAM
                profiles = [
                    "small-1w", "small-1m", "small-6m", "small-1y",
                    "medium-1w", "medium-1m", "medium-6m", "medium-1y",
                    "large-1w"
                ]
            else:
                # Codespace 32GB RAM (standard)
                profiles = [
                    "small-1w", "small-1m",
                    "medium-1w", "medium-1m",
                    "large-1w"
                ]

        print(f"\n{'='*60}")
        print(f"üéØ SUITE S√âQUENTIELLE COMPL√àTE")
        print(f"{'='*60}")
        print(f"RAM d√©tect√©e: {ram_gb} GB")
        print(f"Profils: {len(profiles)}")
        print(f"Tests: {len(profiles)} √ó 3 paradigmes = {len(profiles) * 3} benchmarks")
        print(f"Seed: {seed}")

        # V√©rifier espace
        storage = self._get_storage_info()
        if 'free_gb' in storage:
            print(f"üíæ Stockage: {storage['used_gb']}/{storage['total_gb']} GB ({storage['usage_percent']}%)")

        sessions = []

        for i, profile in enumerate(profiles, start=1):
            print(f"\n{'‚ïê'*60}")
            print(f"[INFO] PROFIL {i}/{len(profiles)}: {profile}")
            print(f"{'‚ïê'*60}")

            session = self.run_full_dataset_benchmark(profile, seed)
            sessions.append(session)

            # R√©sum√©
            status_emoji = "[OK]" if session.status == "completed" else "[WARN]"
            print(f"\n{status_emoji} {profile}: {session.status}")
            if session.errors:
                for error in session.errors:
                    print(f"  [WARN]  {error}")

            # Pause entre profils
            if i < len(profiles):
                print(f"\n‚è≥ Pause de 30s avant profil suivant...")
                time.sleep(30)

        # R√©sum√© final
        print(f"\n{'='*60}")
        print(f"[DONE] SUITE COMPL√àTE TERMIN√âE")
        print(f"{'='*60}")
        print(f"Total: {len(sessions)} profils")
        completed = sum(1 for s in sessions if s.status == "completed")
        failed = sum(1 for s in sessions if s.status == "failed")
        print(f"R√©ussis: {completed}")
        print(f"√âchou√©s: {failed}")
        print(f"\nüìÇ R√©sultats: {self.output_dir}")

        return sessions


# =============================================================================
# CLI
# =============================================================================

def main():
    """Interface CLI pour l'orchestrateur."""
    import sys

    if len(sys.argv) < 2:
        print("Usage: python orchestrator.py <command> [args...]")
        print()
        print("Commands:")
        print("  single <profile> <paradigm>  - Benchmark unique (ex: small-1w postgres)")
        print("  dataset <profile>            - Workflow complet pour un dataset")
        print("  suite [profiles...]          - Suite s√©quentielle compl√®te")
        print()
        print("Exemples:")
        print("  python orchestrator.py single small-1w postgres")
        print("  python orchestrator.py dataset small-1w")
        print("  python orchestrator.py suite small-1w medium-1w")
        print("  python orchestrator.py suite  # Profils par d√©faut (auto-d√©tection RAM)")
        return

    orchestrator = BenchmarkOrchestrator()
    command = sys.argv[1]

    if command == "single":
        if len(sys.argv) < 4:
            print("Usage: orchestrator.py single <profile> <paradigm>")
            return
        profile = sys.argv[2]
        paradigm = sys.argv[3]
        result = orchestrator.run_paradigm_benchmark(profile, paradigm)
        if result:
            print(f"\n[OK] Benchmark termin√©: {paradigm} sur {profile}")

    elif command == "dataset":
        if len(sys.argv) < 3:
            print("Usage: orchestrator.py dataset <profile>")
            return
        profile = sys.argv[2]
        session = orchestrator.run_full_dataset_benchmark(profile)
        print(f"\n[OK] Session termin√©e: {session.status}")

    elif command == "suite":
        profiles = sys.argv[2:] if len(sys.argv) > 2 else None
        sessions = orchestrator.run_sequential_suite(profiles)
        print(f"\n[OK] {len(sessions)} sessions termin√©es")

    else:
        print(f"[ERROR] Commande inconnue: {command}")


if __name__ == "__main__":
    main()
