# Refactoring Consolid√© - BaseType Benchmark

> Document unique fusionnant AUDIT, REFACTORING_SPEC, et STACK
> Align√© sur l'objectif papier: "Un graphe in-memory est-il justifi√© pour les SI b√¢timentaires?"

Date de mise √† jour: 2025-12-22

---

## 1. Objectif et Contexte

### 1.1 Question de recherche (papier.md)

> Un graphe in-memory est-il justifi√© pour les SI b√¢timentaires ?

**Contribution unique**: Premier benchmark b√¢timentaire avec analyse param√©trique co√ªt-m√©moire (RAM comme variable exp√©rimentale)

### 1.2 Livrables scientifiques attendus

| Livrable | Description | Section papier |
|----------|-------------|----------------|
| Matrices RAM √ó Moteur | Latences p95 par profil (small/medium/large) | ¬ß4.5 |
| RAM_min par config | Plus petite RAM sans OOM ni d√©gradation > 20% | ¬ß4.5 |
| Courbes latence = f(RAM) | Visualisation du point d'inflexion | ¬ß4.5 |
| Ratio efficience | Performance / Go allou√© | ¬ß4.5 |

### 1.3 Stack technique

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ORCHESTRATION                                               ‚îÇ
‚îÇ run.py (4000+ lignes) - smoke_benchmark.py                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DATASET                                                     ‚îÇ
‚îÇ generator_v2.py ‚Üí exporter_v2.py ‚Üí dataset_manager.py      ‚îÇ
‚îÇ Format pivot: Parquet (reproductibilit√©)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DOCKER CONTAINERS                                           ‚îÇ
‚îÇ btb_timescaledb ‚îÇ btb_memgraph ‚îÇ btb_oxigraph              ‚îÇ
‚îÇ (P1/P2/M2/O2)   ‚îÇ (M1/M2)      ‚îÇ (O1/O2)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MONITORING                                                  ‚îÇ
‚îÇ cgroup v2: memory.current, memory.peak, cpu.stat           ‚îÇ
‚îÇ Fallback: docker stats (parsing fragile)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. Corrections Effectu√©es

### 2.1 Daily Chunks - BOS Pattern ‚úÖ

**Probl√®me**: Ancien mod√®le = chunks fixes (50 valeurs) ‚Üí 520k chunks ‚Üí M1 loading 3h+

**Solution**: 1 chunk par (point, jour) ‚Üí ~44k chunks ‚Üí ~5 min

| Fichier | Modification |
|---------|--------------|
| `exporter_v2.py` | `DailyChunk`, `generate_daily_chunks()` |
| `run.py` | `_load_memgraph_chunks_csv()` ‚Üí `ArchiveDay` + `HAS_TIMESERIES` |
| `queries/m1/*.cypher` | `TSChunk` ‚Üí `ArchiveDay` |
| `queries/m2/graph/*.cypher` | Idem |

**Justification papier**: Q13 = stress-test dechunking (DOW filter)

### 2.2 P2 Schema Fix ‚úÖ

**Probl√®me**: Colonne `building_id` manquante dans schema JSONB

**Solution**:
```sql
-- Ajout√© dans run.py
CREATE TABLE nodes (
    ...
    building_id TEXT,  -- AJOUT√â
    ...
);
CREATE INDEX idx_nodes_building ON nodes(building_id);
```

### 2.3 Lazy Export/Prune Workflow ‚úÖ

**Probl√®me**: Export complet large-1y > 100GB, d√©passe capacit√© disque

**Solution**:
```
1. generate_parquet_only() ‚Üí Parquet pivot conserv√©
2. Pour chaque sc√©nario:
   a. export_scenario_only() ‚Üí export lazy
   b. run benchmark
   c. prune_scenario() ‚Üí lib√®re disque
3. timeseries.csv partag√©e entre P1/P2/M2/O2
```

| Fichier | M√©thode ajout√©e |
|---------|-----------------|
| `dataset_manager.py` | `generate_parquet_only()` |
| `dataset_manager.py` | `export_scenario_only()` |
| `dataset_manager.py` | `prune_scenario()` |
| `smoke_benchmark.py` | Workflow int√©gr√© |

### 2.4 Auto-reorder Scenarios ‚úÖ

**Probl√®me**: Si M1/O1 avant P2/M2/O2, timeseries.csv prun√©e trop t√¥t

**Solution**:
```python
OPTIMAL_ORDER = ["P1", "P2", "M2", "O2", "M1", "O1"]
scenarios = sorted(scenarios, key=lambda s: OPTIMAL_ORDER.index(s))
```

### 2.5 UX Progress Bars ‚úÖ

**Probl√®me**: Exports longs sans feedback

**Solution**:
```python
def _progress_bar(current, total, prefix="", start_time=None):
    # [=====>......] 1,234/5,000 (24.7%) ETA: 2.3m
```

### 2.6 Monitoring cgroup v2 ‚úÖ

**Probl√®me**: Parsing `docker stats` fragile

**Solution**: Lecture directe cgroup v2
- `memory.current` ‚Üí RAM actuelle
- `memory.peak` ‚Üí RAM max (reset avec sudo -n)
- `cpu.stat` ‚Üí usage_usec

### 2.7 Corrections ant√©rieures ‚úÖ

| Bug | Fix | Commit |
|-----|-----|--------|
| StatusSimulator √©crase `states` | Renommer en `state_values` | 8726340 |
| AlarmSimulator abstrait | Ajouter `step()` trivial | f37d076 |
| Postgres creds mismatch | Lire `docker/.env` | 42a4fae |
| TimescaleDB timeout | Retry budget 60‚Üí120s | 18d4d5c |

---

## 3. Travaux Futurs (Priorit√©s)

### P1: Docker SDK (robustesse monitoring)

**Probl√®me actuel**: Fallback `docker stats` encore utilis√© si cgroup indisponible

**Solution cible**:
```python
import docker
client = docker.from_env()
stats = client.containers.get(name).stats(stream=False)
mem_bytes = stats['memory_stats']['usage']  # Pas de parsing string
```

**Fichiers √† modifier**:
- `run.py` ‚Üí `get_container_stats()`
- `src/basetype_benchmark/benchmark/resource_monitor.py`

**Impact papier**: Robustesse, pas bloquant

### P2: F√©d√©ration Scalable (temp tables)

**Probl√®me actuel**: `WHERE point_id = ANY(ARRAY[id1, id2, ...])` g√©n√®re SQL gigantesque

**Solution cible**:
```python
class FederationHandler:
    BATCH_THRESHOLD = 10000

    def execute_federated_query(self, point_ids, ts_query):
        if len(point_ids) < self.BATCH_THRESHOLD:
            return self._execute_with_array(point_ids, ts_query)
        else:
            return self._execute_with_temp_table(point_ids, ts_query)

    def _execute_with_temp_table(self, point_ids, ts_query):
        cursor.execute("CREATE TEMP TABLE _fed_ids (point_id TEXT)")
        cursor.copy_from(buffer, "_fed_ids")
        # JOIN au lieu de IN(...)
```

**Impact papier**: Requis pour large-1y

### P3: D√©coupage run.py (maintenabilit√©)

**√âtat actuel**: 4000+ lignes, ~85 fonctions, responsabilit√©s m√©lang√©es

**Architecture cible**:
```
src/basetype_benchmark/
‚îú‚îÄ‚îÄ runners/
‚îÇ   ‚îú‚îÄ‚îÄ base.py           # BenchmarkRunner (abstract)
‚îÇ   ‚îú‚îÄ‚îÄ postgres.py       # PostgresRunner (P1/P2)
‚îÇ   ‚îú‚îÄ‚îÄ memgraph.py       # MemgraphRunner (M1/M2)
‚îÇ   ‚îî‚îÄ‚îÄ oxigraph.py       # OxigraphRunner (O1/O2)
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îú‚îÄ‚îÄ docker_manager.py # docker-py API
‚îÇ   ‚îî‚îÄ‚îÄ cgroup_monitor.py # M√©triques cgroup v2
‚îú‚îÄ‚îÄ queries/
‚îÇ   ‚îú‚îÄ‚îÄ loader.py         # load_query, substitute_params
‚îÇ   ‚îî‚îÄ‚îÄ federation.py     # FederationHandler
‚îî‚îÄ‚îÄ workflows/
    ‚îú‚îÄ‚îÄ dataset.py        # workflow_dataset
    ‚îî‚îÄ‚îÄ benchmark.py      # workflow_benchmark
```

**Impact papier**: Maintenabilit√© long terme, pas bloquant pour publication

---

## 4. √âtat du Smoke Test

### 4.1 Sc√©narios

| Sc√©nario | √âtat | Notes |
|----------|------|-------|
| P1 | ‚úÖ OK | p50 = 1.3ms (Q1) |
| P2 | ‚úÖ Corrig√© | building_id ajout√© |
| M1 | üîÑ Pr√™t | Regenerate requis |
| M2 | ‚è≥ Pending | Apr√®s P1/P2 |
| O1 | ‚è≥ Pending | Apr√®s M1 |
| O2 | ‚è≥ Pending | Apr√®s P1/P2 |

### 4.2 M√©triques Daily Chunks

| M√©trique | Ancien | Nouveau |
|----------|--------|---------|
| Chunks M1 | ~520k | ~44k |
| R√©duction | - | ~12x |
| Pattern | TSChunk (50 fixe) | ArchiveDay (1/jour) |

---

## 5. Roadmap

### Phase 1: Baseline reproductible (ACTUEL)

```bash
# Sur B3
git pull
rm -rf src/basetype_benchmark/dataset/exports/small-2d_seed42/
python3 scripts/smoke_benchmark.py --profile small-2d \
  --scenarios P1 P2 M2 O2 M1 O1 --ram-levels 8 \
  --n-warmup 1 --n-runs 1 --queries Q1
```

**Crit√®re**: 6 sc√©narios √ó Q1 = 6 JSON valides

### Phase 2: Campagne small-2d compl√®te

```bash
python3 scripts/smoke_benchmark.py --profile small-2d \
  --scenarios P1 P2 M1 M2 O1 O2 \
  --ram-levels 8 16 32 \
  --n-warmup 3 --n-runs 10 \
  --queries Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13
```

**Livrable**: Premi√®re matrice RAM √ó Moteur

### Phase 3: Extension medium-1m

**Prerequis**: F√©d√©ration temp tables (P2)

### Phase 4: Large-1y

**Prerequis**:
- ‚úÖ Lazy export/prune
- ‚è≥ F√©d√©ration temp tables
- Serveur 128+ Go RAM

---

## 6. Checklist Publication

### Donn√©es

- [ ] Matrices RAM √ó Moteur (small, medium, large)
- [ ] Courbes latence = f(RAM)
- [ ] RAM_min par configuration
- [ ] Ratio efficience

### Reproductibilit√©

- [x] Seed=42 fixe
- [x] Docker Compose versionn√©
- [x] Fingerprint int√©grit√©
- [ ] Scripts publics document√©s
- [ ] README ex√©cution

### Code

- [x] Daily chunks (BOS pattern)
- [x] cgroup v2 monitoring
- [x] Lazy export/prune
- [ ] F√©d√©ration temp tables (P2)
- [ ] Docker SDK (P1)
- [ ] D√©coupage run.py (P3)

---

## 7. Commits R√©cents

```
d1368c7 Auto-reorder scenarios for optimal disk usage
dd23ee3 Lazy export workflow + UX progress + neutrality cleanup
c3d167f Add OVH B3 runbook (diagnostics + Postgres debug)
42a4fae Fix Postgres connection config (read docker/.env)
18d4d5c Increase TimescaleDB connection retry budget
f37d076 Fix AlarmSimulator abstract step implementation
8726340 Fix StatusSimulator state storage
```

---

## 8. Fichiers Obsol√®tes (√† supprimer)

Les documents suivants sont d√©sormais fusionn√©s dans ce fichier:

| Fichier | Contenu migr√© vers |
|---------|-------------------|
| `AUDIT_REFACTORING_SPEC.md` | ¬ß2 (Corrections), ¬ß3 (Priorit√©s) |
| `REFACTORING_SPEC.md` | ¬ß3.P3 (Architecture cible) |
| `REFactoring_AUDIT_vs_SPEC_and_STACK.md` | ¬ß1.3 (Stack), ¬ß3 (Priorit√©s) |
| `SYNTHESIS_SPECS_PAPER.md` | ¬ß1 (Objectif papier), ¬ß6 (Checklist) |
| `PROGRESS_TRACKER.md` | ¬ß2 (Corrections), ¬ß4 (√âtat smoke) |

**Action**: Supprimer ces 5 fichiers apr√®s validation de ce document.
